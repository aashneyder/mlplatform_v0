{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd486ff2-a76f-4f60-9207-6f6a38227812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should run following command for correct connection\n",
    "# export MLFLOW_S3_ENDPOINT_URL=http://ip:9000\n",
    "# export AWS_SECRET_ACCESS_KEY=BudxuXerLsZdP5J7Y4dsifCWCCVtvVdieUwMn927\n",
    "# export AWS_ACCESS_KEY_ID=H0aH9fCBmpmAFIK7GOCF\n",
    "!pip install mlflow mlserver",
    "import mlflow\n",
    "from mlserver.types import InferenceResponse, InferenceRequest\n",
    "from mlserver.codecs.string import StringRequestCodec\n",
    "from requests.models import Response\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f006b9e8-7766-4b65-90b0-1822f33744b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nINFO: needs to give model_name for encode_response, before extract it from request\\n    def encode_response_model(self, model_output: dict) -> InferenceResponse:\\n        _output = json.dumps(model_output)\\n        print(f\"_output = {_output}, тип _output: {type(_output)}\")\\n        _output_dict = json.loads(_output)\\n        print(f\"_output_dict = {_output_dict}, тип _output_dict: {type(_output_dict)}\")\\n        inference_response = StringRequestCodec.encode_response(_output_dict)\\n        print(f\"inference_response = {inference_response}, тип inference_response: {type(inference_response)}\")\\n        return inference_response\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip = \"158.160.52.53\"\n",
    "mlflow.set_tracking_uri(f\"http://{ip}:5000\")\n",
    " \n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, request_input):\n",
    "        print(f\"Полученный predict запрос: {request_input}, тип: {type(request_input)}\")\n",
    "        model_input = self.decode_request(request_input)\n",
    "        print(f\"Результат decode_request для передачи в модель: {model_input}, тип: {type(model_input)}\")\n",
    "        model_output = self.my_custom_function(model_input)\n",
    "        print(f\"Результат работы модели (my_custom_function): {model_output}, тип: {type(model_output)}\")\n",
    "        inference_response = self.encode_response_model(model_output)\n",
    "        print(f\"Обработанный ответ модели для передачи mlserver: {request_input}, тип: {type(request_input)}\")\n",
    "        return inference_response\n",
    " \n",
    "    def my_custom_function(self, model_input: dict) -> dict:\n",
    "        model_output = {\"result\": model_input[\"a\"] * 2}\n",
    "        return model_output\n",
    " \n",
    "    def decode_request(self, model_input: InferenceRequest) -> dict:\n",
    "        raw_json = StringRequestCodec.decode_request(model_input)\n",
    "        print(f\"raw_json = {raw_json}, тип raw_json: {type(raw_json)}\")\n",
    "        print(f\"raw_json[0] = {raw_json[0]}, тип raw_json[0] : {type(raw_json[0])}\")\n",
    "        _input = json.loads(raw_json[0])\n",
    "        return _input\n",
    "        \n",
    "    def encode_response_model(self, model_output: dict) -> dict:\n",
    "        _output = json.dumps(model_output)\n",
    "        print(f\"_output = {_output}, тип _output: {type(_output)}\")\n",
    "        _output_np = np.array(_output, dtype='object')\n",
    "        print(f\"_output_np = {_output_np}, тип _output_np: {type(_output_np)}\")\n",
    "        inference_response = {\"output\": _output_np}\n",
    "        return inference_response\n",
    "\"\"\" \n",
    "INFO: needs to give model_name for encode_response, before extract it from request\n",
    "    def encode_response_model(self, model_output: dict) -> InferenceResponse:\n",
    "        _output = json.dumps(model_output)\n",
    "        print(f\"_output = {_output}, тип _output: {type(_output)}\")\n",
    "        _output_dict = json.loads(_output)\n",
    "        print(f\"_output_dict = {_output_dict}, тип _output_dict: {type(_output_dict)}\")\n",
    "        inference_response = StringRequestCodec.encode_response(_output_dict)\n",
    "        print(f\"inference_response = {inference_response}, тип inference_response: {type(inference_response)}\")\n",
    "        return inference_response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b8416a-8d3d-499c-8f41-e880bcde98db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'work_model1'.\n",
      "2024/02/19 13:16:20 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: work_model1, version 1\n",
      "Created version '1' of model 'work_model1'.\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel()\n",
    "model_path = \"work_model1\"\n",
    "reg_model_name = \"work_model1\"\n",
    "with mlflow.start_run(run_name=\"run_work_model1_v1\") as run:\n",
    "    model_path = f\"{model_path}\"\n",
    "    mlflow.pyfunc.save_model(path=model_path, python_model=my_model)\n",
    "    mlflow.pyfunc.log_model(artifact_path=model_path, python_model=my_model, registered_model_name=reg_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ad7f9-3364-458c-8a5d-9702ef5bff3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda98c49-8414-4b8f-b434-22094b88aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
